# üìë RAG with Cross-Encoders Re-ranking Demo Application

This is a demo project showcasing a Large Language Model (LLM) integrated with Retrieval-Augmented Generation (RAG). The project demonstrates how to utilize LLMs for advanced document retrieval and text generation tasks.

## Demo
[![Watch the Video](https://img.youtube.com/vi/fjWbWOT4VJU/0.jpg)](https://youtu.be/fjWbWOT4VJU)


## Requirements

- Python >= 3.10
- SQLite >= 3.35
- [Ollama](https://ollama.ai/download)

## Setting Up Locally

To set up this project on your local machine, follow these steps:

### 1. Install Python and SQLite

Make sure you have Python 3.10 or greater installed. You can download Python from the official [Python website](https://www.python.org/). Additionally, ensure you have SQLite version 3.35 or higher. SQLite is typically pre-installed with Python, but you can check your version by running:

```bash
sqlite3 --version
```

### 2. Install Dependencies

Make sure you have installed all these dependencies.

```sh
pip install ollama chromadb sentence-transformers streamlit pymupdf langchain-community
```

```sh
ollama pull llama3.2:3b
```

```sh
ollama pull nomic-embed-text
```


### 3. Run the Application

Run this application using the following command:

```sh
ollama run llama3.2
```

```sh
streamlit run app.py
```

### üë©‚Äçüíª Demo the App

We have provided three example review papers in the [papers folder](https://github.com/jd-coderepos/rag-reranker-QA/tree/main/papers) to help you explore the application's capabilities.

#### Step 1: Upload Paper 1

- **Paper Title:** "Ecological restoration in Brazilian biomes: Identifying advances and gaps"
- **Sample Question:**  
  - *"What were the advances identified regarding the ecological restoration in Brazilian biomes in this paper?"*
- **What to Expect:**  
  - View the response generated by the application.  
  - Check the "See retrieved documents" section to understand how the retrieval component functioned.  
  - Examine the "See most relevant document IDs" section to see how the reranker prioritized the retrieved content.

#### Step 2: Upload Paper 2

- **Paper Title:** "Seed germination traits can contribute better to plant community ecology"
- **Important:** The system should now have indexed both Paper 1 and Paper 2.
- **Sample Question:**  
  - *"What methods were used in Paper 1?"*
- **Objective:**  
  - Test whether the reranker can correctly select the relevant content chunks from Paper 1.
- **What to Expect:**  
  - Review the answer provided by the application.  
  - Check the "See retrieved documents" section to analyze retrieval.  
  - Look at the "See most relevant document IDs" section to verify the reranker's performance. Here it should have selected only the chunks corresponding to paper 1.

#### Step 3: Ask a Generic Question

- **Sample Question:**  
  - *"What methods were reported in the paper?"*
- **Expected Behavior:**  
  - If multiple papers are uploaded and the query lacks specificity, the application should not return a response.

#### Step 4: Ask a Question from Paper 2

- **Sample Question:**  
  - *"Do seed germination traits contribute better to plant community ecology?"*
- **What to Expect:**  
  - Review the application's response.  
  - Check the retrieval insights in "See retrieved documents."  
  - Analyze the re-ranking behavior in "See most relevant document IDs."

By following these steps, you can evaluate how well the application handles document retrieval, reranking, and response generation based on your queries.

## License

This project is licensed under the [MIT License](LICENSE).

You are free to use, modify, and distribute this software in accordance with the terms of the license.